test_set = subset(data2[,-1], split == FALSE)
## Normalisation des données
# Création des variables dummies
dummies <- fastDummies::dummy_cols(data2[,-c(1,22)], remove_first_dummy=TRUE, remove_selected_columns=TRUE)
# Nouvelle base de données (data3)
Cible <- data2[,22]
# Calcul des maxs et des mins de chaque variable
maxs <- apply(data3[,-ncol(data3)], 2, max)
# Normalisation par la méthode min_max (scaled=(x-min)/(max-min))
scaled <- as.data.frame(scale(data3[,-ncol(data3)], center = as.numeric(mins), scale = as.numeric(maxs - mins)))
# Nouvelle base de données (data4)
data4 <- cbind(scaled, Cible)
test_set_dum = subset(data4, split == FALSE)
f <- as.formula(paste("Cible ~", paste(n[!n %in% "Cible"], collapse = " + ")))
View(training_set_dum)
View(training_set_dum)
data <- read.csv(params$path)                 # Importation des données
data <- read.csv(params$path)                 # Importation des données
data2 <- data[rowSums(is.na(data)) == 0, -1]  # Retire les lignes constituées de valeurs manquantes uniquement
kable(data2[1:5,])
# Recodage de la variable Cible
data2 <- data2 %>%
mutate(Cible = ifelse(Cible == 1,0,1))
sapply(data2,function(x) sum(is.na(x)))       # Recherche des données manquantes
set.seed(123)
split = sample.split(data2$Cible, SplitRatio = 0.70)
training_set = subset(data2[,-1], split == TRUE)
test_set = subset(data2[,-1], split == FALSE)
View(test_set)
View(test_set)
## Normalisation des données
# Création des variables dummies
dummies <- fastDummies::dummy_cols(data2[,-c(1,22)], remove_first_dummy=TRUE, remove_selected_columns=TRUE)
# Nouvelle base de données (data3)
Cible <- data2[,22]
# Calcul des maxs et des mins de chaque variable
maxs <- apply(data3[,-ncol(data3)], 2, max)
# Nouvelle base de données (data4)
data4 <- cbind(scaled, Cible)
View(data2)
View(data2)
## Normalisation des données
# Création des variables dummies
dummies <- fastDummies::dummy_cols(data2[,-c(21)], remove_first_dummy=TRUE, remove_selected_columns=TRUE)
# Nouvelle base de données (data3)
Cible <- data2[,21]
data3 <- cbind(dummies, Cible)
# Calcul des maxs et des mins de chaque variable
maxs <- apply(data3[,-ncol(data3)], 2, max)
mins <- apply(data3[,-ncol(data3)], 2, min)
# Normalisation par la méthode min_max (scaled=(x-min)/(max-min))
scaled <- as.data.frame(scale(data3[,-ncol(data3)], center = as.numeric(mins), scale = as.numeric(maxs - mins)))
# Nouvelle base de données (data4)
data4 <- cbind(scaled, Cible)
#
training_set_dum = subset(data4, split == TRUE)
test_set_dum = subset(data4, split == FALSE)
n <- names(training_set_dum)
f <- as.formula(paste("Cible ~", paste(n[!n %in% "Cible"], collapse = " + ")))
#Courbe ROC du meilleur modèle
model_pmc <- neuralnet(f,data=training_set_dum, hidden=1, linear.output=T,  threshold=0.8)
## Test
# Selection de toutes les variables sauf la cible
sub_2=subset(test_set_dum,select=c(1:61))
head(sub_2)
# Calcul de je sais pas quoi
nn.results2=compute(model_pmc,sub_2)
# Réel vs prédiction
results_2=data.frame(actuel=test_set_dum$Cible,prediction=nn.results2$net.result)
head(results_2)
# On round les prédictions pour avoir que des 0 et des 1
res_2_arr=sapply(results_2,round,digits=0)
attach(res_2_arr_df)
res_2_arr_df=data.frame(res_2_arr)
attach(res_2_arr_df)
attach(res_2_arr_df)
mean <- mean(actuel == prediction)
mean
table(actuel,prediction)# matrice de confusion
p_pmc <- predict(model_pmc, newdata=subset(test_set_dum,select=c(1:61)), type="response")
pr_pmc <- ROCR::prediction(p_pmc, test_set_dum$Cible)
prf_pmc <- performance(pr_pmc, measure = "tpr", x.measure = "fpr")
plot(prf_pmc, col="blue")
# AUC
auc_pmc <− performance (pr_pmc ,  measure = "auc" )
auc_pmc <− auc_pmc@y.values[[1]]
auc_pmc
plot(prf_pmc, col="blue")
# Création du modèle
# Recodage de la variable cible pour faire des arbres de décisions
Cible2_train <- ifelse(training_set$Cible==1,"Defaut","Non_defaut")
train2 <- data.frame(training_set[,-20],Cible2_train)
Cible2_test <- ifelse(test_set$Cible==1,"Defaut","Non_defaut")
test2 <- data.frame(test_set[,-20],Cible2_test)
# Création du modèle
model_rf <- randomForest(Cible2_train ~ ., data=train2, method="class", mtry=4, importance=TRUE)
# Création du modèle
model_rf <- randomForest(Cible2_train ~ ., data=train2, method="class", mtry=4, importance=TRUE)
model_rf
model_rf <- randomForest(Cible2_train ~ ., data=train2, method="class", mtry=15, ntree=95, importance=TRUE)
## Courbe ROC
# Calcul des prédictions
prediction_for_roc_curve <- predict(model_rf, test2[,-20],type="prob")
# Spécification des classes
classes <- levels(test2$Cible2_test)
# Definitiond des classes
true_values <- ifelse(test2[,20]==classes[1],1,0)
# Performances des prédictions
pred_rf <- ROCR::prediction(prediction_for_roc_curve[,1],true_values)
prf_rf <- performance(pred, "tpr", "fpr")
# Performances des prédictions
pred_rf <- ROCR::prediction(prediction_for_roc_curve[,1],true_values)
prf_rf <- performance(pred_rf, "tpr", "fpr")
plot(prf_rf, col="green")
plot(prf_pmc, add = TRUE, col="blue")
legend(0.7, 0.2, legend=c("Random Forest", "PMC"),
col=c("green", "blue"), lty=c(1,1), cex=0.8)
# On recode la variable cible comme une variable de type factor
data2$Cible = factor(data2$Cible, levels = c(0, 1))
# Création des variables dummies
dummies <- fastDummies::dummy_cols(data2[,-c(1,ncol(data2))], remove_first_dummy=TRUE, remove_selected_columns=TRUE)
# Nouvelle base de données (data3)
Cible <- data2[,ncol(data2)]
data3 <- cbind(dummies, Cible)
# Constitution des échantillons train et test
set.seed(123)
split = sample.split(data3$Cible, SplitRatio = 0.70)
training_set_svm = subset(data3, split == TRUE)
# Normalisation des données
training_set_svm[-ncol(data3)] = scale(training_set_svm[-ncol(data3)])
# Création du modèle
a=c(0,0,0)
# Création du modèle logit
model_logit <- glm(Cible ~., family=binomial(link='logit'), data=training_set)
# Résultats
summary(model_logit)
# Résultats
summary(model_logit)
# Evaluation de la capacité prédictive du modèle
fitted.results <- predict(model_logit, newdata=subset(test_set,select=c(1:20)), type='response')
fitted.results <- ifelse(fitted.results > 0.5,1,0)
misClasificError <- mean(fitted.results != test_set$Cible)
print(paste('Accuracy',1-misClasificError))
# Courbe ROC du meilleur modèle
p_logit <- predict(model_logit, newdata=subset(test_set,select=c(1:20)), type="response")
pr_logit <- ROCR::prediction(p_logit, test_set$Cible)
prf_logit <- performance(pr_logit, measure = "tpr", x.measure = "fpr")
plot(prf_logit, col="red")
plot(prf_pmc, add = TRUE, col="blue")
plot(prf_logit, col="red")
plot(prf_pmc, add = TRUE, col="blue")
legend(0.8, 0.2, legend=c("Logit", "PMC"),
col=c("red", "blue"), lty=c(1,1), cex=0.8)
auc_logit <− performance (pr_logit ,  measure = "auc" )
auc_logit <− auc_logit@y.values[[1]]
auc_logit
print(paste('Accuracy',1-misClasificError))
# On recode la variable cible comme une variable de type factor
data2$Cible = factor(data2$Cible, levels = c(0, 1))
# Création des variables dummies
dummies <- fastDummies::dummy_cols(data2[,-c(1,ncol(data2))], remove_first_dummy=TRUE, remove_selected_columns=TRUE)
data3 <- cbind(dummies, Cible)
# Constitution des échantillons train et test
set.seed(123)
training_set_svm = subset(data3, split == TRUE)
# Normalisation des données
training_set_svm[-ncol(data3)] = scale(training_set_svm[-ncol(data3)])
test_set_svm[-ncol(data3)] = scale(test_set_svm[-ncol(data3)])
training_set_svm = subset(data3, split == TRUE)
test_set_svm = subset(data3, split == FALSE)
# Normalisation des données
training_set_svm[-ncol(data3)] = scale(training_set_svm[-ncol(data3)])
test_set_svm[-ncol(data3)] = scale(test_set_svm[-ncol(data3)])
# Meilleur modèle
model_svm = svm(formula = Cible ~ ., data = training_set_svm, type = 'C-classification', kernel = 'linear',cost=4, epsilon=0.1)
#Courbe ROC du meilleur modèle
p_svm <- predict(model_svm, newdata=subset(test_set_svm,select=c(1:61)), type="response")
pr_svm <- ROCR::prediction(as.numeric(p_svm), as.numeric(test_set_svm$Cible))
data <- read.csv(params$path)                 # Importation des données
# Recodage de la variable Cible
data2 <- data2 %>%
mutate(Cible = ifelse(Cible == 1,0,1))
sapply(data2,function(x) sum(is.na(x)))       # Recherche des données manquantes
set.seed(123)
split = sample.split(data2$Cible, SplitRatio = 0.70)
test_set = subset(data2[,-1], split == FALSE)
# On recode la variable cible comme une variable de type factor
data2$Cible = factor(data2$Cible, levels = c(0, 1))
# Création des variables dummies
dummies <- fastDummies::dummy_cols(data2[,-c(1,ncol(data2))], remove_first_dummy=TRUE, remove_selected_columns=TRUE)
View(data2)
View(data2)
# Création des variables dummies
dummies <- fastDummies::dummy_cols(data2[,-c(ncol(data2))], remove_first_dummy=TRUE, remove_selected_columns=TRUE)
# Nouvelle base de données (data3)
Cible <- data2[,ncol(data2)]
data3 <- cbind(dummies, Cible)
# Constitution des échantillons train et test
set.seed(123)
split = sample.split(data3$Cible, SplitRatio = 0.70)
training_set_svm = subset(data3, split == TRUE)
test_set_svm = subset(data3, split == FALSE)
# Normalisation des données
training_set_svm[-ncol(data3)] = scale(training_set_svm[-ncol(data3)])
test_set_svm[-ncol(data3)] = scale(test_set_svm[-ncol(data3)])
# Meilleur modèle
model_svm = svm(formula = Cible ~ ., data = training_set_svm, type = 'C-classification', kernel = 'linear',cost=4, epsilon=0.1)
#Courbe ROC du meilleur modèle
p_svm <- predict(model_svm, newdata=subset(test_set_svm,select=c(1:61)), type="response")
pr_svm <- ROCR::prediction(as.numeric(p_svm), as.numeric(test_set_svm$Cible))
prf_svm <- performance(pr_svm, measure = "tpr", x.measure = "fpr")
plot(prf_svm, col="orange")
plot(prf_pmc, add = TRUE, col="blue")
legend(0.7, 0.2, legend=c("SVM", "PMC"),
col=c("orange", "blue"), lty=c(1,1), cex=0.8)
pr_svm <- ROCR::prediction(p_svm, test_set_svm$Cible)
plot(prf_svm, col="orange")
plot(prf_pmc, add = TRUE, col="blue")
legend(0.7, 0.2, legend=c("SVM", "PMC"),
col=c("orange", "blue"), lty=c(1,1), cex=0.8)
# Calcul de l'AUC
auc_svm <− performance (pred_svm ,  measure = "auc" )
# Calcul de l'AUC
auc_svm <− performance (pr_svm ,  measure = "auc" )
auc_svm <− auc_svm@y.values[[1]]
auc_svm
# Matrice de confusion
actuel=test_set_svm[, ncol(test_set_svm)]
prev=pred_svm
# Prédictions
pred_svm = predict(model_svm, newdata = test_set_svm[-ncol(test_set_svm)])
# Matrice de confusion
actuel=test_set_svm[, ncol(test_set_svm)]
prev=pred_svm
confusion_table = table(actuel, prev) # On a 74.6 bonnes classifications
mean=mean(actuel==prev)
mean
auc_rf <− performance (pred_rf ,  measure = "auc" )
auc_rf <− auc_rf@y.values[[1]]
auc_rf
predValid <- predict(model_rf, test2, type="class")
mean <- mean(predValid == test2$Cible2_test)
mean
auc_pmc <− performance (pr_pmc ,  measure = "auc" )
auc_pmc <− auc_pmc@y.values[[1]]
auc_pmc
#Courbe ROC du meilleur modèle
model_pmc <- neuralnet(f,data=training_set_dum, hidden=1, linear.output=T,  threshold=0.8)
## Test
# Selection de toutes les variables sauf la cible
sub_2=subset(test_set_dum,select=c(1:61))
head(sub_2)
# Calcul de je sais pas quoi
nn.results2=compute(model_pmc,sub_2)
# Réel vs prédiction
results_2=data.frame(actuel=test_set_dum$Cible,prediction=nn.results2$net.result)
head(results_2)
# On round les prédictions pour avoir que des 0 et des 1
res_2_arr=sapply(results_2,round,digits=0)
attach(res_2_arr_df)
mean <- mean(actuel == prediction)
mean
data <- read.csv(params$path)                 # Importation des données
# Recodage de la variable Cible
data2 <- data2 %>%
mutate(Cible = ifelse(Cible == 1,0,1))
data <- read.csv(params$path)                 # Importation des données
data2 <- data[rowSums(is.na(data)) == 0, -1]  # Retire les lignes constituées de valeurs manquantes uniquement
# Recodage de la variable Cible
data2 <- data2 %>%
mutate(Cible = ifelse(Cible == 1,0,1))
sapply(data2,function(x) sum(is.na(x)))       # Recherche des données manquantes
set.seed(123)
split = sample.split(data2$Cible, SplitRatio = 0.70)
training_set = subset(data2[,-1], split == TRUE)
test_set = subset(data2[,-1], split == FALSE)
## Normalisation des données
# Création des variables dummies
dummies <- fastDummies::dummy_cols(data2[,-c(21)], remove_first_dummy=TRUE, remove_selected_columns=TRUE)
# Nouvelle base de données (data3)
Cible <- data2[,21]
data3 <- cbind(dummies, Cible)
# Calcul des maxs et des mins de chaque variable
maxs <- apply(data3[,-ncol(data3)], 2, max)
mins <- apply(data3[,-ncol(data3)], 2, min)
# Nouvelle base de données (data4)
data4 <- cbind(scaled, Cible)
#
training_set_dum = subset(data4, split == TRUE)
# Normalisation par la méthode min_max (scaled=(x-min)/(max-min))
scaled <- as.data.frame(scale(data3[,-ncol(data3)], center = as.numeric(mins), scale = as.numeric(maxs - mins)))
# Nouvelle base de données (data4)
data4 <- cbind(scaled, Cible)
#
training_set_dum = subset(data4, split == TRUE)
test_set_dum = subset(data4, split == FALSE)
n <- names(training_set_dum)
f <- as.formula(paste("Cible ~", paste(n[!n %in% "Cible"], collapse = " + ")))
#Courbe ROC du meilleur modèle
model_pmc <- neuralnet(f,data=training_set_dum, hidden=1, linear.output=T,  threshold=0.8)
## Test
# Selection de toutes les variables sauf la cible
sub_2=subset(test_set_dum,select=c(1:61))
head(sub_2)
# Calcul de je sais pas quoi
nn.results2=compute(model_pmc,sub_2)
# Réel vs prédiction
results_2=data.frame(actuel=test_set_dum$Cible,prediction=nn.results2$net.result)
head(results_2)
# On round les prédictions pour avoir que des 0 et des 1
res_2_arr=sapply(results_2,round,digits=0)
res_2_arr_df=data.frame(res_2_arr)
attach(res_2_arr_df)
mean <- mean(actuel == prediction)
table(actuel,prediction)# matrice de confusion
mean
p_pmc <- predict(model_pmc, newdata=subset(test_set_dum,select=c(1:61)), type="response")
prf_pmc <- performance(pr_pmc, measure = "tpr", x.measure = "fpr")
p_pmc <- predict(model_pmc, newdata=subset(test_set_dum,select=c(1:61)), type="response")
pr_pmc <- ROCR::prediction(p_pmc, test_set_dum$Cible)
prf_pmc <- performance(pr_pmc, measure = "tpr", x.measure = "fpr")
plot(prf_pmc, col="blue")
knitr::opts_chunk$set(echo = TRUE)
# Packages utilisés
library(knitr)
library(dplyr)          # Pour la manipulation des données
library(dplyr)          # Pour la manipulation des données
library(ggplot2)        # Pour les graphiques
library(ggplot2)        # Pour les graphiques
library(caTools)        # Pour constituer les train et test samples (fonction sample.split())
library(fastDummies)    # Pour remplacer les variables qualitatives par des dummies (cas du SVM)
library(ROCR)           # Pour tracer des courbes ROC
#library(pROC)           # Pour tracer les courbes ROC (RandomForest)
library(PredictABEL)
library(ROCR)           # Pour tracer des courbes ROC
#library(pROC)           # Pour tracer les courbes ROC (RandomForest)
library(PredictABEL)
library(randomForest)   # Pour le Random Forest
#library(pROC)           # Pour tracer les courbes ROC (RandomForest)
library(PredictABEL)
library(randomForest)   # Pour le Random Forest
library(e1071)          # Pour le SVM
data <- read.csv(params$path)                 # Importation des données
data2 <- data[rowSums(is.na(data)) == 0, -1]  # Retire les lignes constituées de valeurs manquantes uniquement
# Recodage de la variable Cible
data2 <- data2 %>%
mutate(Cible = ifelse(Cible == 1,0,1))
sapply(data2,function(x) sum(is.na(x)))       # Recherche des données manquantes
set.seed(123)
split = sample.split(data2$Cible, SplitRatio = 0.70)
training_set = subset(data2[,-1], split == TRUE)
test_set = subset(data2[,-1], split == FALSE)
## Normalisation des données
# Création des variables dummies
dummies <- fastDummies::dummy_cols(data2[,-c(21)], remove_first_dummy=TRUE, remove_selected_columns=TRUE)
# Nouvelle base de données (data3)
Cible <- data2[,21]
data3 <- cbind(dummies, Cible)
# Calcul des maxs et des mins de chaque variable
maxs <- apply(data3[,-ncol(data3)], 2, max)
mins <- apply(data3[,-ncol(data3)], 2, min)
# Normalisation par la méthode min_max (scaled=(x-min)/(max-min))
scaled <- as.data.frame(scale(data3[,-ncol(data3)], center = as.numeric(mins), scale = as.numeric(maxs - mins)))
# Nouvelle base de données (data4)
data4 <- cbind(scaled, Cible)
#
training_set_dum = subset(data4, split == TRUE)
test_set_dum = subset(data4, split == FALSE)
n <- names(training_set_dum)
f <- as.formula(paste("Cible ~", paste(n[!n %in% "Cible"], collapse = " + ")))
#Courbe ROC du meilleur modèle
model_pmc <- neuralnet(f,data=training_set_dum, hidden=1, linear.output=T,  threshold=0.8)
library(neuralnet)      # Pour le PMC
#Courbe ROC du meilleur modèle
model_pmc <- neuralnet(f,data=training_set_dum, hidden=1, linear.output=T,  threshold=0.8)
## Test
# Selection de toutes les variables sauf la cible
sub_2=subset(test_set_dum,select=c(1:61))
head(sub_2)
# Calcul de je sais pas quoi
nn.results2=compute(model_pmc,sub_2)
# Réel vs prédiction
results_2=data.frame(actuel=test_set_dum$Cible,prediction=nn.results2$net.result)
head(results_2)
# On round les prédictions pour avoir que des 0 et des 1
res_2_arr=sapply(results_2,round,digits=0)
res_2_arr_df=data.frame(res_2_arr)
attach(res_2_arr_df)
mean <- mean(actuel == prediction)
table(actuel,prediction)# matrice de confusion
mean
p_pmc <- predict(model_pmc, newdata=subset(test_set_dum,select=c(1:61)), type="response")
pr_pmc <- ROCR::prediction(p_pmc, test_set_dum$Cible)
prf_pmc <- performance(pr_pmc, measure = "tpr", x.measure = "fpr")
plot(prf_pmc, col="blue")
# AUC
auc_pmc <− performance (pr_pmc ,  measure = "auc" )
auc_pmc <− auc_pmc@y.values[[1]]
auc_pmc
# Création du modèle
# Recodage de la variable cible pour faire des arbres de décisions
Cible2_train <- ifelse(training_set$Cible==1,"Defaut","Non_defaut")
train2 <- data.frame(training_set[,-20],Cible2_train)
Cible2_test <- ifelse(test_set$Cible==1,"Defaut","Non_defaut")
test2 <- data.frame(test_set[,-20],Cible2_test)
# Création du modèle
model_rf <- randomForest(Cible2_train ~ ., data=train2, method="class", mtry=4, importance=TRUE)
# Création du modèle
model_rf <- randomForest(Cible2_train ~ ., data=train2, method="class", mtry=4, importance=TRUE)
model_rf
# Prédictions sur l'échantillon d'apprentissage
predTrain <- predict(model_rf, train2, type="class")
model_rf <- randomForest(Cible2_train ~ ., data=train2, method="class", mtry=15, ntree=95, importance=TRUE)
## Courbe ROC
# Calcul des prédictions
prediction_for_roc_curve <- predict(model_rf, test2[,-20],type="prob")
# Spécification des classes
classes <- levels(test2$Cible2_test)
# Definitiond des classes
true_values <- ifelse(test2[,20]==classes[1],1,0)
# Performances des prédictions
pred_rf <- ROCR::prediction(prediction_for_roc_curve[,1],true_values)
prf_rf <- performance(pred_rf, "tpr", "fpr")
plot(prf_rf, col="green")
plot(prf_pmc, add = TRUE, col="blue")
legend(0.7, 0.2, legend=c("Random Forest", "PMC"),
col=c("green", "blue"), lty=c(1,1), cex=0.8)
auc_rf <− performance (pred_rf ,  measure = "auc" )
auc_rf <− auc_rf@y.values[[1]]
auc_rf
knitr::opts_chunk$set(echo = TRUE)
# Packages utilisés
library(knitr)
library(dplyr)          # Pour la manipulation des données
library(ggplot2)        # Pour les graphiques
library(caTools)        # Pour constituer les train et test samples (fonction sample.split())
library(ggplot2)        # Pour les graphiques
library(caTools)        # Pour constituer les train et test samples (fonction sample.split())
library(fastDummies)    # Pour remplacer les variables qualitatives par des dummies (cas du SVM)
library(ROCR)           # Pour tracer des courbes ROC
#library(pROC)           # Pour tracer les courbes ROC (RandomForest)
library(PredictABEL)
library(randomForest)   # Pour le Random Forest
library(e1071)          # Pour le SVM
library(neuralnet)      # Pour le PMC
data <- read.csv(params$path)                 # Importation des données
data2 <- data[rowSums(is.na(data)) == 0, -1]  # Retire les lignes constituées de valeurs manquantes uniquement
# Recodage de la variable Cible
data2 <- data2 %>%
mutate(Cible = ifelse(Cible == 1,0,1))
sapply(data2,function(x) sum(is.na(x)))       # Recherche des données manquantes
## Constitution des échantillons train et test
defauts <- data2 %>%
filter(Cible==1)                   # 30% de défauts
non_defauts <- data2 %>%
filter(Cible==0)               # 70% de non-défauts
train <-data2[1:700, ]                        # Echantillon d'apprentissage (70% des données)
test <- data2[701:nrow(data2),]               # Echantillon test (30% des données)
set.seed(123)
split = sample.split(data2$Cible, SplitRatio = 0.70)
training_set = subset(data2[,-1], split == TRUE)
test_set = subset(data2[,-1], split == FALSE)
# On recode la variable cible comme une variable de type factor
data2$Cible = factor(data2$Cible, levels = c(0, 1))
# Création des variables dummies
dummies <- fastDummies::dummy_cols(data2[,-c(ncol(data2))], remove_first_dummy=TRUE, remove_selected_columns=TRUE)
# Nouvelle base de données (data3)
Cible <- data2[,ncol(data2)]
data3 <- cbind(dummies, Cible)
# Constitution des échantillons train et test
set.seed(123)
split = sample.split(data3$Cible, SplitRatio = 0.70)
training_set_svm = subset(data3, split == TRUE)
test_set_svm = subset(data3, split == FALSE)
# Normalisation des données
training_set_svm[-ncol(data3)] = scale(training_set_svm[-ncol(data3)])
test_set_svm[-ncol(data3)] = scale(test_set_svm[-ncol(data3)])
# Meilleur modèle
model_svm = svm(formula = Cible ~ ., data = training_set_svm, type = 'C-classification', kernel = 'linear',cost=4, epsilon=0.1)
#Courbe ROC du meilleur modèle
p_svm <- predict(model_svm, newdata=subset(test_set_svm,select=c(1:61)), type="response")
pr_svm <- ROCR::prediction(as.numeric(p_svm), as.numeric(test_set_svm$Cible))
prf_svm <- performance(pr_svm, measure = "tpr", x.measure = "fpr")
plot(prf_svm, col="orange")
plot(prf_pmc, add = TRUE, col="blue")
knitr::opts_chunk$set(echo = FALSE)
library(forecast)
knitr::opts_chunk$set(echo = FALSE)
library(forecast)
library(forecast)
install.packages("forecast")
install.packages("forecast")
library(forecast)
library(forecast)
source("C:/Users/Utilisateur/Documents/M1_SEMESTRE_2/Shiny/Langevin_Julien/Series_temporelles.R")
load("C:/Users/Utilisateur/Documents/M1_SEMESTRE_2/Shiny/Langevin_Julien/TS.RData")
TS=get(params$TS)
start=eval(parse(text=params$start))
end=eval(parse(text=params$end))
t0=eval(parse(text=params$t0))
t02=eval(parse(text=params$t02))
plot(TS)
sim = simul_ts(start,end,params$f,params$a,params$b,params$Type,params$p,params$c,params$s)
moymob(sim$X, params$p)
knitr::opts_chunk$set(echo = FALSE)
library(forecast)
mm=moymob(sim$X, params$p)
mm=moymob(sim$X, params$p)
sais(sim[["X"]],mm,params$Type,params$p)
shiny::runApp('C:/Users/Utilisateur/Desktop/Appli_SVM/app')
runApp('C:/Users/Utilisateur/Desktop/Appli_SVM/app')
shiny::runApp()
